{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Modules ### \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta \n",
    "from pycoingecko import CoinGeckoAPI\n",
    "\n",
    "### GCP Modules ### \n",
    "from google.cloud import storage \n",
    "import gcsfs\n",
    "import glob\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/gabrielgomes/Downloads/awaricripto-5cfe64a9c26d.json\"\n",
    "\n",
    "# Establish Client connection to the project \n",
    "project = 'awaricripto'\n",
    "client  = storage.Client(project=project)\n",
    "fs = gcsfs.GCSFileSystem(project=project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read GS files without downloading it\n",
    "def fs_to_notebook_transfer(id):\n",
    "    with fs.open(f'{id}_awaricripto_bucket/{id}_ts') as f:\n",
    "        id_ts = pd.read_csv(f, index_col = 'Unnamed: 0')\n",
    "    return id_ts \n",
    "\n",
    "# -----\n",
    "\n",
    "# Updating workflow function\n",
    "def dailyExtractor(id):\n",
    "    ''' Retrieves latest metrics from coinGeckoAPI to a given 'id' | creating the 1-line df: 'id_metrics_today'\n",
    "    '''\n",
    "    # Set retrieving date\n",
    "    today = datetime.now().strftime('%d-%m-%Y') # 15-03-2022 \n",
    "    yesterday = ( datetime.now() - timedelta(days=1)).strftime('%d-%m-%Y') # 14-03-2022 \n",
    "    \n",
    "    # Set API connection cg variable\n",
    "    cg = CoinGeckoAPI()\n",
    "        \n",
    "    # Set request dictionary\n",
    "    request_dict = cg.get_coin_history_by_id(id = id, date = today)\n",
    "        \n",
    "    # Request community data from API ##############################################\n",
    "    rqst_community_data     = request_dict['community_data']\n",
    "    twitter_foll            = rqst_community_data.get('twitter_followers')\n",
    "    reddit_subs             = rqst_community_data.get('reddit_subscribers')\n",
    "    reddit_avg_posts_48h    = rqst_community_data.get('reddit_average_posts_48h')\n",
    "    reddit_avg_comments_48h = rqst_community_data.get('reddit_average_comments_48h')\n",
    "\n",
    "    # Request development data from API ##############################################\n",
    "    rqst_developer_data       = request_dict['developer_data']\n",
    "    forks                     = rqst_developer_data.get('forks')\n",
    "    stars                     = rqst_developer_data.get('stars')\n",
    "    subscribers               = rqst_developer_data.get('subscribers')\n",
    "    total_issues              = rqst_developer_data.get('total_issues')\n",
    "    closed_issues             = rqst_developer_data.get('closed_issues')\n",
    "    pull_rqst_merged          = rqst_developer_data.get('pull_requests_merged')\n",
    "    pull_request_contributors = rqst_developer_data.get('pull_request_contributors')\n",
    "        \n",
    "    # Request market data from API ##############################################\n",
    "        \n",
    "    # Price \n",
    "    rqst_price_data   = request_dict['market_data']['current_price']\n",
    "    usd_current_price = rqst_price_data.get('usd')\n",
    "    eur_current_price = rqst_price_data.get('eur')\n",
    "    brl_current_price = rqst_price_data.get('brl')\n",
    "        \n",
    "    # Market cap\n",
    "    rqst_mktcap_data = request_dict['market_data']['market_cap']\n",
    "    usd_market_cap   = rqst_mktcap_data.get('usd')\n",
    "    eur_market_cap   = rqst_mktcap_data.get('eur')\n",
    "    brl_market_cap   = rqst_mktcap_data.get('brl')\n",
    "        \n",
    "    # Total volume \n",
    "    rqst_volume_data = request_dict['market_data']['total_volume']\n",
    "    usd_total_volume = rqst_volume_data.get('usd')\n",
    "    eur_total_volume = rqst_volume_data.get('eur')\n",
    "    brl_total_volume = rqst_volume_data.get('brl')\n",
    "\n",
    "    # Set dataframe using the lists\n",
    "    extracted_df = pd.DataFrame({\n",
    "        \n",
    "                               # date\n",
    "                               'dates':today,\n",
    "            \n",
    "                               # community data\n",
    "                               'twitter_followers':twitter_foll,\n",
    "                               'reddit_subs':reddit_subs,\n",
    "                               'reddit_avg_posts_48h':reddit_avg_posts_48h,\n",
    "                               'reddit_avg_comments_48h':reddit_avg_comments_48h,\n",
    "\n",
    "                               # developer data        \n",
    "                               'forks':forks,\n",
    "                               'stars':stars,\n",
    "                               'github_subs':subscribers,\n",
    "                               'total_issues':total_issues,\n",
    "                               'closed_issues':closed_issues,\n",
    "                               'pull_rqst_merged':pull_rqst_merged,\n",
    "                               'pull_request_contributors':pull_request_contributors,\n",
    "                         \n",
    "                               # current_price data\n",
    "                               'usd_cp':usd_current_price,\n",
    "                               'eur_cp':eur_current_price,\n",
    "                               'brl_cp':brl_current_price,\n",
    "\n",
    "                               # Market_cap data\n",
    "                               'usd_mc':usd_market_cap,\n",
    "                               'eur_mc':eur_market_cap,\n",
    "                               'brl_mc':brl_market_cap,\n",
    "\n",
    "                               # total_volume data\n",
    "                               'usd_tv':usd_total_volume,\n",
    "                               'eur_tv':eur_total_volume,\n",
    "                               'brl_tv':brl_total_volume},\n",
    "                                \n",
    "                               # pass index\n",
    "                                index = [0])\n",
    "    \n",
    "    # Rounding lists\n",
    "    round_zero_list = ['twitter_followers','reddit_subs',\n",
    "                       'forks','stars','github_subs',\n",
    "                       'total_issues','closed_issues',\n",
    "                       'pull_rqst_merged','pull_request_contributors',\n",
    "                       'usd_mc', 'eur_mc', 'brl_mc',\n",
    "                       'usd_tv', 'eur_tv', 'brl_tv']\n",
    "    \n",
    "    round_two_list = [col for col in extracted_df.columns if col not in round_zero_list]\n",
    "    \n",
    "    # Rounding\n",
    "    extracted_df[round_zero_list] = round(extracted_df[round_zero_list])\n",
    "    extracted_df[round_two_list] = round(extracted_df[round_two_list],2)\n",
    "    \n",
    "    # Return\n",
    "    return extracted_df\n",
    "# Ok, function set\n",
    "\n",
    "# ------\n",
    "\n",
    "# Define function 'concater()'\n",
    "def concater(id, today):\n",
    "    \n",
    "    # Extract todays data\n",
    "    # Read local file of outdated time series for each id\n",
    "    # Concat the 2 files\n",
    "    # Back fill None, in case there is any unreatrieved metrics\n",
    "    id_today    = dailyExtractor(id)\n",
    "    id_outdated = fs_to_notebook_transfer(id)\n",
    "    id_completed  = pd.concat([id_today, id_outdated], ignore_index=True)  \n",
    "    id_completed  = id_completed.fillna(method='bfill')\n",
    "    \n",
    "    # Download it \n",
    "    id_completed.to_csv(today + f'/{id}_completed.csv')\n",
    "# Ok, we can use concater to update the time series\n",
    "\n",
    "# ------\n",
    "\n",
    "# Set function to upload '{id}_updated' to Google Cloud Storage 'id' bucket, replacing (updating) it.\n",
    "def upload_completed(id, today):\n",
    "    \n",
    "    bucket_name           = f'{id}_awaricripto_bucket'\n",
    "    source_file_name      = today + f'/{id}_completed.csv'\n",
    "    destination_blob_name = f'{id}_ts'\n",
    "    \n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob   = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(f'{destination_blob_name} with content from {source_file_name} uploaded to {bucket_name}.\\n')\n",
    "# Ok, we can use upload_updated to upload the updated time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read main50\n",
    "with fs.open('main50/main50.csv') as f:\n",
    "        main50 = pd.read_csv(f, index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply functions \n",
    "* Iterate over ids in main50\n",
    "    * concater(id) | to retrieve latest updated metrics, concatenate it with outdated data ({id}_ts_pvm.csv), and save it in the pvm as '{id}_completed.csv' \n",
    "    * upload_completed(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date-based folder | Facilitate organization\n",
    "today = datetime.now().strftime('%d-%m-%Y')\n",
    "os.makedirs(today, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate for ids\n",
    "for id in main50.id:\n",
    "\n",
    "    # Retrieve latest updated metrics and save it in the pvm as 'date/{id}_completed.csv' \n",
    "    concater(id, today)\n",
    "    \n",
    "    # Upload '{id}_completed.csv' to GS\n",
    "    upload_completed(id, today)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
