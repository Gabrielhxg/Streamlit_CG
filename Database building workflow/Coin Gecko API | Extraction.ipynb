{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Requirements ### \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pycoingecko import CoinGeckoAPI\n",
    "cg = CoinGeckoAPI()\n",
    "\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cgAPI Usage test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command\n",
    "* cg.get_coins_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cg command\n",
    "all_coins = cg.get_coins_list()\n",
    "\n",
    "# Get a list of all 'ids'\n",
    "id_list = [each['id'] for each in all_coins]\n",
    "\n",
    "# Get a list of all 'symbols'\n",
    "symbol_list = [each['symbol'] for each in all_coins]\n",
    "\n",
    "# Dataframe of both \n",
    "all_tokens = pd.DataFrame(symbol_list, index = id_list, columns = ['symbol']) \n",
    "\n",
    "# Drop first row (the token has id = '', blocking request accesses through 'id', cast it off)\n",
    "all_tokens.drop(index=all_tokens.index[0], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01coin</th>\n",
       "      <td>zoc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-5x-long-algorand-token</th>\n",
       "      <td>algohalf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-5x-long-altcoin-index-token</th>\n",
       "      <td>althalf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-5x-long-ascendex-token-token</th>\n",
       "      <td>asdhalf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0-5x-long-bitcoin-cash-token</th>\n",
       "      <td>bchhalf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zynecoin</th>\n",
       "      <td>zyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyro</th>\n",
       "      <td>zyro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zytara-dollar</th>\n",
       "      <td>zusd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyx</th>\n",
       "      <td>zyx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzz-finance-v2</th>\n",
       "      <td>zzzv2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12656 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  symbol\n",
       "01coin                               zoc\n",
       "0-5x-long-algorand-token        algohalf\n",
       "0-5x-long-altcoin-index-token    althalf\n",
       "0-5x-long-ascendex-token-token   asdhalf\n",
       "0-5x-long-bitcoin-cash-token     bchhalf\n",
       "...                                  ...\n",
       "zynecoin                             zyn\n",
       "zyro                                zyro\n",
       "zytara-dollar                       zusd\n",
       "zyx                                  zyx\n",
       "zzz-finance-v2                     zzzv2\n",
       "\n",
       "[12656 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check index unicity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ each for each in all_tokens.index.value_counts() if each > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we have 'id' and 'symbol' for 12,656 tradable crypto tokens.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate for all coins \n",
    "* We want to find the **top 1000 crypto projects** in terms of twitter followers.\n",
    "* To do it, we need to go over the 12k tokens: \n",
    "    * check if they have **'community_data'** available\n",
    "    * check if they have **'twitter_followers'** available (is not None)\n",
    "    \n",
    "#### API limit considerations\n",
    "* CoinGecko allows up to 50 requests per minute, meaning\n",
    "    * We need to reduce **all_coins** to 253 50-sized chunks \n",
    "    * Request **get_coin_history_by_id(id='', date='today')** for each token 'id'\n",
    "    * Access **request_dict['community_data']**, and check if **rqst_community_data.get('twitter_followers') is not None**\n",
    "    * if it **is not None**, append 'twitter_followers' to the received df \n",
    "    * (it will take 253 minutes...)\n",
    "    \n",
    "---    \n",
    "    \n",
    "### First approach\n",
    "* Chunk down **all_tokens** (index = 'id', columns = ['symbol']) in 253 slots of 50 tokens\n",
    "* Define a function which \n",
    "    * takes one chunck, get data from the API request\n",
    "    * Adds it (if not None) to the received df and returns said df as 'extracted_df' \n",
    "* **run function 253 times**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking it down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk contains 50 tokens\n"
     ]
    }
   ],
   "source": [
    "# Chunk splits\n",
    "chunk_list = np.array_split(all_tokens, 253)\n",
    "\n",
    "# Check chunk size\n",
    "test_chunk = chunk_list[39]\n",
    "print(f'Chunk contains {test_chunk.shape[0]} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extractor(df):\n",
    "    \n",
    "    '''\n",
    "    * For each token in df, calls for an API request, \n",
    "    access 'community_data' and 'developer_data' from request, \n",
    "    get metrics from data dicts,\n",
    "    returns df with indexed 'twitter_followers', 'reddit_subscribers'\n",
    "                            'forks', 'stars', 'subscribers', \n",
    "                            'total_issues', 'closed_issues'\n",
    "    \n",
    "    \n",
    "    Idea:\n",
    "    twitterExtractor(tokens_chunk)\n",
    "    out: df['id','twitter_followers', 'reddit_subscribers', \n",
    "            'forks', 'stars', 'subscribers', 'total_issues', 'closed_issues'  ]\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # set today's date\n",
    "    #today = datetime.utcnow().strftime('%d-%m-%Y')\n",
    "    today = '17-02-2022'\n",
    "    \n",
    "    # set appendable lists\n",
    "    ids_list = []\n",
    "    twitter_foll_list = []\n",
    "    reddit_subs_list = [] \n",
    "    forks_list = []\n",
    "    stars_list = []\n",
    "    subscribers_list = []\n",
    "    total_issues_list = []\n",
    "    closed_issues_list = []\n",
    "\n",
    "    # loop\n",
    "    for id in df.index:\n",
    "    \n",
    "        # make request \n",
    "        request_dict = cg.get_coin_history_by_id(id = id, date = today)\n",
    "    \n",
    "        # check if request_dict contains 'community_data' and 'developer_data'\n",
    "        if ('community_data' in request_dict.keys()) and ('developer_data' in request_dict.keys()):\n",
    "        \n",
    "            # access community and developer data dicts in request\n",
    "            rqst_community_data = request_dict['community_data']\n",
    "            rqst_developer_data = request_dict['developer_data']\n",
    "    \n",
    "            # COMM METRICS \n",
    "            # get 'twitter_followers' from community_data dict\n",
    "            twitter_foll = rqst_community_data.get('twitter_followers')\n",
    "            \n",
    "            # get 'reddit_subscribers' from community_data dict\n",
    "            reddit_subs = rqst_community_data.get('reddit_subscribers')\n",
    "            \n",
    "            # GITHUB METRICS \n",
    "            # get 'forks' from developer_data dict\n",
    "            forks = rqst_developer_data.get('forks')\n",
    "            \n",
    "            # get 'stars' from developer_data dict\n",
    "            stars = rqst_developer_data.get('stars')\n",
    "            \n",
    "            # get 'subscribers' from developer_data dict\n",
    "            subscribers = rqst_developer_data.get('subscribers')\n",
    "            \n",
    "            # get 'total_issues' from developer_data dict\n",
    "            total_issues = rqst_developer_data.get('total_issues')\n",
    "            \n",
    "            # get 'closed_issues' from developer_data dict\n",
    "            closed_issues = rqst_developer_data.get('closed_issues')\n",
    "            \n",
    "            \n",
    "            # if twitter_foll is not None, append all values to respective lists\n",
    "            if twitter_foll is not None:\n",
    "                ids_list.append(id)\n",
    "                twitter_foll_list.append(twitter_foll)\n",
    "                reddit_subs_list.append(reddit_subs)\n",
    "                forks_list.append(forks)\n",
    "                stars_list.append(stars)\n",
    "                subscribers_list.append(subscribers)\n",
    "                total_issues_list.append(total_issues)\n",
    "                closed_issues_list.append(closed_issues)\n",
    "    \n",
    "    # Set dataframe using the lists\n",
    "    extrected_df = pd.DataFrame({'id':ids_list,\n",
    "                               'twitter_followers':twitter_foll_list,\n",
    "                               'reddit_subs':reddit_subs_list,\n",
    "                               'forks':forks_list,\n",
    "                               'stars':stars_list,\n",
    "                               'github_subs':subscribers_list,\n",
    "                               'total_issues':total_issues_list,\n",
    "                               'closed_issues':closed_issues_list\n",
    "                              })            \n",
    "    return extrected_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>twitter_followers</th>\n",
       "      <th>reddit_subs</th>\n",
       "      <th>forks</th>\n",
       "      <th>stars</th>\n",
       "      <th>github_subs</th>\n",
       "      <th>total_issues</th>\n",
       "      <th>closed_issues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brewlabs</td>\n",
       "      <td>8437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brick-token</td>\n",
       "      <td>5134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bridge</td>\n",
       "      <td>7584</td>\n",
       "      <td>337.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bridge-mutual</td>\n",
       "      <td>45821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bright-token</td>\n",
       "      <td>9907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bright-union</td>\n",
       "      <td>28452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bring-finance</td>\n",
       "      <td>4829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>brokoli</td>\n",
       "      <td>110461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>brother-music-platform</td>\n",
       "      <td>5431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bscarmy</td>\n",
       "      <td>1281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bscbond</td>\n",
       "      <td>37843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bsccrop</td>\n",
       "      <td>7175</td>\n",
       "      <td>37.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bscgold</td>\n",
       "      <td>8598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bsclaunch</td>\n",
       "      <td>76226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bscshiba</td>\n",
       "      <td>5702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bscstarter</td>\n",
       "      <td>122304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bsc-station</td>\n",
       "      <td>359831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bscview</td>\n",
       "      <td>8693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bsocial</td>\n",
       "      <td>56145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>btcmoon</td>\n",
       "      <td>1078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>btc-standard-hashrate-token</td>\n",
       "      <td>67047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bt-finance</td>\n",
       "      <td>1691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>btmiracles</td>\n",
       "      <td>57349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  twitter_followers  reddit_subs forks stars  \\\n",
       "0                      brewlabs               8437          NaN  None  None   \n",
       "1                   brick-token               5134          NaN  None  None   \n",
       "2                        bridge               7584        337.0  None  None   \n",
       "3                 bridge-mutual              45821          NaN  None  None   \n",
       "4                  bright-token               9907          NaN  None  None   \n",
       "5                  bright-union              28452          NaN  None  None   \n",
       "6                 bring-finance               4829          NaN  None  None   \n",
       "7                       brokoli             110461          NaN  None  None   \n",
       "8        brother-music-platform               5431          NaN  None  None   \n",
       "9                       bscarmy               1281          NaN  None  None   \n",
       "10                      bscbond              37843          NaN  None  None   \n",
       "11                      bsccrop               7175         37.0  None  None   \n",
       "12                      bscgold               8598          NaN  None  None   \n",
       "13                    bsclaunch              76226          NaN  None  None   \n",
       "14                     bscshiba               5702          NaN  None  None   \n",
       "15                   bscstarter             122304          NaN  None  None   \n",
       "16                  bsc-station             359831          NaN  None  None   \n",
       "17                      bscview               8693          NaN  None  None   \n",
       "18                      bsocial              56145          NaN  None  None   \n",
       "19                      btcmoon               1078          NaN  None  None   \n",
       "20  btc-standard-hashrate-token              67047          NaN  None  None   \n",
       "21                   bt-finance               1691          NaN  None  None   \n",
       "22                   btmiracles              57349          NaN  None  None   \n",
       "\n",
       "   github_subs total_issues closed_issues  \n",
       "0         None         None          None  \n",
       "1         None         None          None  \n",
       "2         None         None          None  \n",
       "3         None         None          None  \n",
       "4         None         None          None  \n",
       "5         None         None          None  \n",
       "6         None         None          None  \n",
       "7         None         None          None  \n",
       "8         None         None          None  \n",
       "9         None         None          None  \n",
       "10        None         None          None  \n",
       "11        None         None          None  \n",
       "12        None         None          None  \n",
       "13        None         None          None  \n",
       "14        None         None          None  \n",
       "15        None         None          None  \n",
       "16        None         None          None  \n",
       "17        None         None          None  \n",
       "18        None         None          None  \n",
       "19        None         None          None  \n",
       "20        None         None          None  \n",
       "21        None         None          None  \n",
       "22        None         None          None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results for test chunck \n",
    "Extractor(test_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **As we dont want to spend 4 hours manually running the extractor function 253 times, we should schedule the function to run on its own every x seconds**\n",
    "---    \n",
    "    \n",
    "### Second approach\n",
    "* Define another function to run the extractor function on every id of every chunk, from 0 to 252, every x seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scheduling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeExtractor(chunk_list, sleep):\n",
    "    '''\n",
    "    chunk_list = list, of all chunked dataframes.\n",
    "    sleep = int, seconds sleeping.\n",
    "    '''    \n",
    "    # set test lenth (2% of all coins)\n",
    "    chunk_list_lenth = 252\n",
    "    \n",
    "    # set 'chunk_list' lenth\n",
    "    #chunk_list_lenth = len(chunk_list)\n",
    "    \n",
    "    # generate list_of_twitterExtracted_dfs\n",
    "    # not comprehensive as we need to communicate the steps\n",
    "    list_of_Extracted_dfs = []\n",
    "    \n",
    "    # timed loop for generating list of dfs\n",
    "    for i in range(chunk_list_lenth):\n",
    "        \n",
    "        # append processing_df to list\n",
    "        list_of_Extracted_dfs.append(Extractor(chunk_list[i]))\n",
    "        \n",
    "        # Communicate process\n",
    "        print(f'Chunk {i} successfully appended to list_of_Extracted_dfs.')\n",
    "        \n",
    "        # condition to sleep\n",
    "        if (i+1) != chunk_list_lenth:\n",
    "            \n",
    "            # Communicate process\n",
    "            print(f'Wait {sleep} seconds before processing next chunk. \\n')\n",
    "        \n",
    "            # wait sleep seconds to rerun Extractor\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "        else:\n",
    "            print('All done.')\n",
    "            \n",
    "    \n",
    "    # Concatanate and return\n",
    "    return pd.concat(list_of_Extracted_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With **executeExtraction**, we've successfully automatized the twitter_followers data collection.\n",
    "\n",
    "* If we are to request **twitter_followers** from the **community_data** request, we might as well extract other things.\n",
    "\n",
    "\n",
    "    * 'community_data': \n",
    "        * 'twitter_followers'\n",
    "        * 'reddit_subscribers'\n",
    "\n",
    "     * 'developer_data' | Github: \n",
    "         * 'forks'\n",
    "         * 'stars'\n",
    "         * 'subscribers'\n",
    "         * 'total_issues'\n",
    "         * 'closed_issues'\n",
    "         \n",
    "     * Market data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api.coingecko.com/api/v3/coins/10084-grayton/history?date=17-02-2022",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4e3e225a73c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_coins_Extracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecuteExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-2e4788c7f241>\u001b[0m in \u001b[0;36mexecuteExtractor\u001b[0;34m(chunk_list, sleep)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# append processing_df to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlist_of_Extracted_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Communicate process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0de48589277a>\u001b[0m in \u001b[0;36mExtractor\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# make request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coin_history_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# check if request_dict contains 'community_data' and 'developer_data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pycoingecko/utils.py\u001b[0m in \u001b[0;36minput_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minput_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pycoingecko/api.py\u001b[0m in \u001b[0;36mget_coin_history_by_id\u001b[0;34m(self, id, date, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__api_url_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunc_args_preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pycoingecko/api.py\u001b[0m in \u001b[0;36m__request\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api.coingecko.com/api/v3/coins/10084-grayton/history?date=17-02-2022"
     ]
    }
   ],
   "source": [
    "all_coins_Extracted = executeExtractor(chunk_list, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As the cell above shows, a '429 Client Error' was raised while requesting API access to the 39th chunk. \n",
    "* Huge problem here, over an hour into the process and we lost the information for all 38 chunks processed before. \n",
    "   * To avoid Errors (of any type) breaking the collection process, lets add some Exception handling.\n",
    "   \n",
    "   \n",
    "**First Exceptions Approach**\n",
    "* Communicate, set to sleep\n",
    "   * Any type of Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fix_executeExtractor(chunk_list, sleep):\n",
    "    '''\n",
    "    chunk_list = list, of all chunked dataframes.\n",
    "    sleep = int, seconds sleeping.\n",
    "    '''    \n",
    "    # set test lenth (2% of all coins)\n",
    "    chunk_list_lenth = 252\n",
    "    \n",
    "    # set 'chunk_list' lenth\n",
    "    #chunk_list_lenth = len(chunk_list)\n",
    "    \n",
    "    # generate list_of_twitterExtracted_dfs\n",
    "    # not comprehensive as we need to communicate the steps\n",
    "    list_of_Extracted_dfs = []\n",
    "    \n",
    "    # timed loop for generating list of dfs\n",
    "    for i in range(chunk_list_lenth):\n",
    "        while True:\n",
    "            \n",
    "            # Try to request\n",
    "            try:\n",
    "                # append processing_df to list\n",
    "                list_of_Extracted_dfs.append(Extractor(chunk_list[i]))\n",
    "            \n",
    "            # Handle error\n",
    "            except Exception as error:\n",
    "                \n",
    "                # Communicate error and handle\n",
    "                print(f'{error} \\n Program will wait for a minute and try again. \\n')\n",
    "                time.sleep(60)\n",
    "                continue\n",
    "                \n",
    "            # if no error, break while loop, continue    \n",
    "            break\n",
    "        \n",
    "        # Communicate process\n",
    "        print(f'Chunk {i} successfully appended to list_of_Extracted_dfs.')\n",
    "        \n",
    "        # condition to sleep\n",
    "        if (i+1) != chunk_list_lenth:\n",
    "            \n",
    "            # Communicate process\n",
    "            print(f'Wait {sleep} seconds before processing next chunk. \\n')\n",
    "        \n",
    "            # wait sleep seconds to rerun Extractor\n",
    "            time.sleep(sleep)\n",
    "        \n",
    "        else:\n",
    "            print('All done.')\n",
    "            \n",
    "    \n",
    "    # Concatanate and return\n",
    "    return pd.concat(list_of_Extracted_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 Client Error: Too Many Requests for url: https://api.coingecko.com/api/v3/coins/01coin/history?date=17-02-2022 \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 0 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 1 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 2 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 3 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 4 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 5 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 6 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 7 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 8 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 9 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "HTTPSConnectionPool(host='api.coingecko.com', port=443): Read timed out. (read timeout=120) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 10 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 11 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 12 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 13 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 14 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 15 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 16 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 17 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 18 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 19 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 20 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 21 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 22 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 23 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "429 Client Error: Too Many Requests for url: https://api.coingecko.com/api/v3/coins/bamboo-coin/history?date=17-02-2022 \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "429 Client Error: Too Many Requests for url: https://api.coingecko.com/api/v3/coins/baby-white-hamster/history?date=17-02-2022 \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 24 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 25 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 26 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 27 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 28 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 29 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 30 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 31 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 32 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 33 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 34 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 35 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 36 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 37 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 38 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 39 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 40 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 41 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 42 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 43 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 44 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 45 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 46 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 47 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 48 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 49 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 50 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 51 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 52 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 53 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 54 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 55 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 56 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 57 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 58 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 59 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 60 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 61 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 62 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 63 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 64 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 65 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 66 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(65, 'EHOSTUNREACH')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 67 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "HTTPSConnectionPool(host='api.coingecko.com', port=443): Read timed out. (read timeout=120) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 68 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 69 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 70 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 71 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 72 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 73 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 74 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 75 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 76 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 77 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 78 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 79 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 80 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 81 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 82 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 83 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 84 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 85 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 86 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 87 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 88 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 89 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 90 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 91 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 92 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 93 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 94 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 95 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 96 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 97 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 98 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 99 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 100 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 101 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 102 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 103 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 104 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 105 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 106 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 107 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 108 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 109 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 110 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 111 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 112 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 113 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 114 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 115 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 116 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 117 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 118 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 119 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 120 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 121 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 122 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 123 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 124 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 125 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 126 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 127 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 128 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 129 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 130 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 131 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 132 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 133 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 134 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 135 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 136 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 137 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 138 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 139 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 140 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 141 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 142 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 143 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 144 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 145 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 146 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 147 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 148 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 149 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 150 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 151 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 152 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 153 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 154 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 155 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 156 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 157 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 158 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 159 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 160 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 161 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 162 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 163 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 164 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 165 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 166 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 167 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 168 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 169 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 170 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 171 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 172 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 173 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 174 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 175 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 176 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 177 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 178 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 179 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 180 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 181 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 182 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 183 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(65, 'EHOSTUNREACH')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 184 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 185 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 186 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "HTTPSConnectionPool(host='api.coingecko.com', port=443): Read timed out. (read timeout=120) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 187 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 188 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 189 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 190 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 191 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 192 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 193 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 194 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 195 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 196 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 197 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 198 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 199 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 200 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 201 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 202 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 203 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 204 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 205 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 206 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 207 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 208 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 209 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 210 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 211 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 212 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 213 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 214 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 215 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 216 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 217 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 218 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 219 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 220 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 221 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 222 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 223 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 224 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 225 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 226 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 227 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 228 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 229 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 230 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 231 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 232 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 233 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 234 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 235 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 236 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 237 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 238 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 239 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 240 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 241 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 242 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 243 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "('Connection aborted.', OSError(\"(54, 'ECONNRESET')\")) \n",
      " Program will wait for a minute and try again. \n",
      "\n",
      "Chunk 244 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 245 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 246 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 247 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 248 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 249 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 250 successfully appended to list_of_Extracted_dfs.\n",
      "Wait 65 seconds before processing next chunk. \n",
      "\n",
      "Chunk 251 successfully appended to list_of_Extracted_dfs.\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "extracted = Fix_executeExtractor(chunk_list,65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>twitter_followers</th>\n",
       "      <th>reddit_subs</th>\n",
       "      <th>forks</th>\n",
       "      <th>stars</th>\n",
       "      <th>github_subs</th>\n",
       "      <th>total_issues</th>\n",
       "      <th>closed_issues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10084-grayton</td>\n",
       "      <td>29577</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10604-somerset</td>\n",
       "      <td>29577</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10612-somerset</td>\n",
       "      <td>29577</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10616-mckinney</td>\n",
       "      <td>29577</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10629-mckinney</td>\n",
       "      <td>29577</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4902</th>\n",
       "      <td>zionomics</td>\n",
       "      <td>2442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4903</th>\n",
       "      <td>zipmex-token</td>\n",
       "      <td>7037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4904</th>\n",
       "      <td>ziticoin</td>\n",
       "      <td>982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4905</th>\n",
       "      <td>zkspace</td>\n",
       "      <td>112163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4906</th>\n",
       "      <td>zlot</td>\n",
       "      <td>4650</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4907 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  twitter_followers reddit_subs forks stars github_subs  \\\n",
       "0      10084-grayton              29577        None  None  None        None   \n",
       "1     10604-somerset              29577        None  None  None        None   \n",
       "2     10612-somerset              29577        None  None  None        None   \n",
       "3     10616-mckinney              29577        None  None  None        None   \n",
       "4     10629-mckinney              29577        None  None  None        None   \n",
       "...              ...                ...         ...   ...   ...         ...   \n",
       "4902       zionomics               2442         NaN   NaN   NaN         NaN   \n",
       "4903    zipmex-token               7037         NaN   NaN   NaN         NaN   \n",
       "4904        ziticoin                982         NaN   NaN   NaN         NaN   \n",
       "4905         zkspace             112163         NaN   NaN   NaN         NaN   \n",
       "4906            zlot               4650         NaN   NaN   NaN         NaN   \n",
       "\n",
       "     total_issues closed_issues  \n",
       "0            None          None  \n",
       "1            None          None  \n",
       "2            None          None  \n",
       "3            None          None  \n",
       "4            None          None  \n",
       "...           ...           ...  \n",
       "4902          NaN           NaN  \n",
       "4903          NaN           NaN  \n",
       "4904          NaN           NaN  \n",
       "4905          NaN           NaN  \n",
       "4906          NaN           NaN  \n",
       "\n",
       "[4907 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 1000 in twitter_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>twitter_followers</th>\n",
       "      <th>reddit_subs</th>\n",
       "      <th>forks</th>\n",
       "      <th>stars</th>\n",
       "      <th>github_subs</th>\n",
       "      <th>total_issues</th>\n",
       "      <th>closed_issues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>ethdown</td>\n",
       "      <td>7701962</td>\n",
       "      <td>801536.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>ethup</td>\n",
       "      <td>7701962</td>\n",
       "      <td>801559.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>coinbase-stock</td>\n",
       "      <td>4845643</td>\n",
       "      <td>200016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>coinbase-stock-bittrex</td>\n",
       "      <td>4845643</td>\n",
       "      <td>200017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>bitcoin</td>\n",
       "      <td>4694036</td>\n",
       "      <td>3914728.0</td>\n",
       "      <td>31613.0</td>\n",
       "      <td>61947.0</td>\n",
       "      <td>3905.0</td>\n",
       "      <td>6730.0</td>\n",
       "      <td>6116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>hydra-token</td>\n",
       "      <td>50179</td>\n",
       "      <td>521.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>diamond-boyz-coin</td>\n",
       "      <td>50178</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>warrior-token</td>\n",
       "      <td>50177</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>ardana</td>\n",
       "      <td>50058</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>only1</td>\n",
       "      <td>50015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  twitter_followers reddit_subs    forks    stars  \\\n",
       "1593                 ethdown            7701962    801536.0      NaN      NaN   \n",
       "1620                   ethup            7701962    801559.0      NaN      NaN   \n",
       "988           coinbase-stock            4845643    200016.0      NaN      NaN   \n",
       "989   coinbase-stock-bittrex            4845643    200017.0      NaN      NaN   \n",
       "628                  bitcoin            4694036   3914728.0  31613.0  61947.0   \n",
       "...                      ...                ...         ...      ...      ...   \n",
       "2133             hydra-token              50179       521.0      2.0     15.0   \n",
       "1330       diamond-boyz-coin              50178         NaN      4.0      4.0   \n",
       "4630           warrior-token              50177         NaN      NaN      NaN   \n",
       "354                   ardana              50058      1101.0      NaN      NaN   \n",
       "3130                   only1              50015         NaN      NaN      NaN   \n",
       "\n",
       "     github_subs total_issues closed_issues  \n",
       "1593         NaN          NaN           NaN  \n",
       "1620         NaN          NaN           NaN  \n",
       "988          NaN          NaN           NaN  \n",
       "989          NaN          NaN           NaN  \n",
       "628       3905.0       6730.0        6116.0  \n",
       "...          ...          ...           ...  \n",
       "2133         4.0          1.0           0.0  \n",
       "1330         1.0          0.0           0.0  \n",
       "4630         NaN          NaN           NaN  \n",
       "354          NaN          NaN           NaN  \n",
       "3130         NaN          NaN           NaN  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_TF = extracted.sort_values(by='twitter_followers', ascending = False).iloc[:1000]\n",
    "topk_TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Considerations**\n",
    "* If you actually check the number of twitter followers for ethdown in the coingecko website, you'll find it links to Binance twitter account (as ethdown is a derivative housed by Binance). \n",
    "\n",
    "* In the next script of this workflow, we'll filter-out derivative tokens (futures, options, perpetuals, swaps),\n",
    "* as they are not crypto projects in and of themselves. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
